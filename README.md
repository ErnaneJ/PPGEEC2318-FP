# üß† Classifica√ß√£o de Imagens com CNNs (Intel Image Dataset)

Este projeto tem como objetivo treinar e avaliar diferentes arquiteturas de Redes Neurais Convolucionais (CNNs) para a tarefa de classifica√ß√£o de imagens. Utilizamos o dataset "Intel Image Classification", que cont√©m imagens de cenas naturais divididas em 6 categorias.

### Autores

- Ernane Ferreira Rocha Junior  
- Quelita M√≠riam Nunes Ferraz

## üìÅ Estrutura do Projeto

```bash
part-1/
‚îÇ
‚îú‚îÄ‚îÄ assets/                       # Gr√°ficos principais 
‚îú‚îÄ‚îÄ download-dataset.py/          # Script para download autom√°tico do Intel Image Dataset 
‚îú‚îÄ‚îÄ part1_base_model.ipynb/       # Etapas do experimento, modelos e avalia√ß√µes 
‚îî‚îÄ‚îÄ README.md/                    # Documenta√ß√£o do projeto
```

## üìä Dataset: Intel Image Classification
Este projeto utiliza o conjunto de dados Intel Image Classification, originalmente disponibilizado por Puneet Bansal no [Kaggle](https://www.kaggle.com/datasets/puneet6060/intel-image-classification).

O dataset cont√©m cerca de 25.000 imagens coloridas com resolu√ß√£o padr√£o de 150x150 pixels. As imagens representam seis categorias distintas de ambientes naturais, conforme listadas abaixo:

* buildings ‚Äì imagens de pr√©dios e constru√ß√µes urbanas
* forest ‚Äì paisagens de floresta
* glacier ‚Äì forma√ß√µes de gelo e geleiras
* mountain ‚Äì imagens de montanhas
* sea ‚Äì paisagens mar√≠timas e costeiras
* street ‚Äì cenas urbanas de ruas

### üìÅ Estrutura dos Dados
O conjunto est√° organizado em tr√™s subconjuntos principais:

```bash
dataset/intel-image-classification/
‚îÇ
‚îú‚îÄ‚îÄ seg_train/        # Dados de treino (~14.000 imagens)
‚îÇ   ‚îú‚îÄ‚îÄ buildings/
‚îÇ   ‚îú‚îÄ‚îÄ forest/
‚îÇ   ‚îú‚îÄ‚îÄ glacier/
‚îÇ   ‚îú‚îÄ‚îÄ mountain/
‚îÇ   ‚îú‚îÄ‚îÄ sea/
‚îÇ   ‚îî‚îÄ‚îÄ street/
‚îÇ
‚îú‚îÄ‚îÄ seg_test/         # Dados de teste (~3.000 imagens rotuladas)
‚îÇ   ‚îú‚îÄ‚îÄ buildings/
‚îÇ   ‚îú‚îÄ‚îÄ forest/
‚îÇ   ‚îú‚îÄ‚îÄ glacier/
‚îÇ   ‚îú‚îÄ‚îÄ mountain/
‚îÇ   ‚îú‚îÄ‚îÄ sea/
‚îÇ   ‚îî‚îÄ‚îÄ street/
‚îÇ
‚îî‚îÄ‚îÄ seg_pred/         # Dados de predi√ß√£o (~7.000 imagens sem r√≥tulo)
```

-----

## ‚öôÔ∏è Arquiteturas dos Modelos

Foram implementadas e testadas tr√™s arquiteturas de CNN principais, com algumas varia√ß√µes no n√∫mero de filtros.

### 1. SimpleCNN

√â um modelo base com uma arquitetura sequencial simples:

  * 3 blocos de `Conv2d` -> `ReLU` -> `MaxPool2d`
  * Uma camada `Flatten` seguida por duas camadas `Linear` (`Dense`) para a classifica√ß√£o final.
  * **Varia√ß√µes Testadas:**
      * **Menos filtros:** Canais `(8, 16, 32)`
      * **Base:** Canais `(16, 32, 64)`
      * **Mais filtros:** Canais `(32, 64, 128)`

### 2. SimpleCNNWithBlocks

Uma evolu√ß√£o do modelo base, adicionando t√©cnicas de regulariza√ß√£o para combater o overfitting:

  * Cada bloco convolucional agora cont√©m: `Conv2d` -> `BatchNorm2d` -> `ReLU` -> `MaxPool2d` -> `Dropout(0.25)`
  * A camada de classifica√ß√£o tamb√©m inclui uma camada `Dropout(0.5)`.
  * O **Batch Normalization** (`BatchNorm2d`) ajuda a estabilizar e acelerar o treinamento, enquanto o **Dropout** desativa neur√¥nios aleatoriamente para evitar que o modelo memorize os dados de treino.

### 3. EfficientStrideCNN

Uma arquitetura alternativa que substitui as camadas de `MaxPool2d` por convolu√ß√µes com `stride=2`. Esta √© uma abordagem mais moderna para reduzir a dimens√£o espacial dos mapas de caracter√≠sticas.

  * Utiliza convolu√ß√µes com `kernel_size=5` e `stride=2` nas camadas iniciais.
  * Finaliza a parte convolucional com uma camada `AdaptiveAvgPool2d`, que adapta a sa√≠da para um tamanho fixo ($1 times 1$), tornando o modelo mais flex√≠vel a diferentes tamanhos de entrada.

-----

## üöÄ Metodologia de Treinamento

  * Foi utilizada uma classe `Architecture` para encapsular a l√≥gica de treinamento, valida√ß√£o, salvamento e plotagem, garantindo reprodutibilidade.
  * Otimizador: `Adam`.
  * `CrossEntropyLoss`, adequada para tarefas de classifica√ß√£o multiclasse.
  * Todos os modelos foram treinados por **10 √©pocas**.

### Defini√ß√£o da Taxa de Aprendizagem (Learning Rate Finder)

A **taxa de aprendizagem (learning rate)** √© um dos hiperpar√¢metros mais cr√≠ticos no treinamento de redes neurais. Ela controla o tamanho do passo que o otimizador d√° na dire√ß√£o contr√°ria ao gradiente da perda.

  * Uma **taxa muito alta** pode fazer com que o modelo "salte" sobre o ponto de m√≠nimo, levando a um treinamento inst√°vel ou √† diverg√™ncia (a perda explode).
  * Uma **taxa muito baixa** torna o treinamento excessivamente lento e pode fazer com que o modelo fique preso em um m√≠nimo local sub√≥timo.

Para evitar a escolha arbitr√°ria deste valor, utilizamos a t√©cnica **Learning Rate Range Test**. O m√©todo consiste em treinar o modelo por um pequeno n√∫mero de itera√ß√µes, come√ßando com uma taxa de aprendizagem muito baixa e aumentando-a exponencialmente a cada passo. Plotamos a perda (Loss) em fun√ß√£o da taxa de aprendizagem (Learning Rate) para analisar o comportamento do modelo.

![Learning Rate Graph](./assets/learning-rate.png)

**An√°lise da Curva:**
O gr√°fico acima mostra o resultado do nosso teste. Podemos observar tr√™s fases distintas:

1.  **Regi√£o Inicial ($10^{-5}$ a ~$10^{-4}$):** A perda permanece quase constante. A taxa de aprendizagem √© t√£o baixa que o modelo mal consegue aprender.
2.  **Regi√£o de Queda Acentuada (~$10^{-4}$ a ~$10^{-2}$):** A perda come√ßa a cair rapidamente. Esta √© a **zona ideal** para escolher nossa taxa de aprendizagem, pois o modelo est√° aprendendo de forma eficiente e est√°vel.
3.  **Regi√£o de Explos√£o (ap√≥s ~$10^{-2}$):** A perda atinge um valor m√≠nimo e depois "explode", subindo drasticamente. Aqui, a taxa de aprendizagem tornou-se alta demais, desestabilizando o treinamento.

**Resultados Obtidos e Escolha:**
O nosso script identificou dois pontos de interesse:

  * `Melhor LR (fundo do U): 0.004154`: O ponto onde a perda foi m√≠nima. Usar este valor pode ser arriscado, pois est√° no limite da estabilidade.
  * `LR seguro antes do fundo: 0.000527`: Um ponto seguro na regi√£o de queda acentuada.

Com base nesta an√°lise, a escolha de uma taxa de aprendizagem de **`1e-3` (ou `0.001`)** para os experimentos principais √© totalmente justificada. Este valor est√° localizado bem no centro da regi√£o de queda acentuada, garantindo um treinamento r√°pido e est√°vel.

-----

## üìä Resultados e An√°lise

Os resultados ap√≥s 10 √©pocas de treinamento foram consolidados na tabela abaixo:

| Modelo | Canais | Val Loss | Par√¢metros |
| :--- | :--- | :---: | ---: |
| **Menos filtros 8-16-32** | (8, 16, 32) | **0,5781** | 1.334.038 |
| **Base 16-32-64** | (16, 32, 64) | 0,5855 | 2.678.694 |
| **Mais filtros 32-64-128** | (32, 64, 128) | 0,6428 | 5.402.566 |
| **EfficientStrideCNN** | (16, 32, 64) | 0,6531 | **50.726** |
| **Base + Blocos (BN/Dropout)** | (16, 32, 64) | 0,7726 | 2.678.918 |

### An√°lise dos Resultados:

-  **Melhor Desempenho:** Surpreendentemente, o modelo **`SimpleCNN` com menos filtros** (`8-16-32`) obteve o menor *Validation Loss* (`0,5781`). Isso sugere que, para este dataset e com 10 √©pocas de treino, uma arquitetura mais simples √© mais eficaz e menos propensa a overfitting.

-  **Overfitting √© Vis√≠vel:** Os gr√°ficos dos modelos `SimpleCNN` (Base e Mais Filtros) mostram um claro sinal de **overfitting**. A perda de treino (linha azul) continua a diminuir drasticamente, enquanto a perda de valida√ß√£o (linha laranja) se estabiliza ou come√ßa a aumentar por volta da 5¬™ √©poca. Isso significa que o modelo est√° memorizando os dados de treino em vez de aprender a generalizar.

-  **Efic√°cia da Regulariza√ß√£o:** O modelo `Base + Blocos (BN/Dropout)` apresentou uma perda de valida√ß√£o maior. No entanto, seu gr√°fico mostra que as curvas de treino e valida√ß√£o est√£o muito mais pr√≥ximas. Isso indica que o **`BatchNorm` e o `Dropout` foram eficazes em reduzir o overfitting**. Provavelmente, este modelo precisaria de mais √©pocas para convergir para um resultado melhor, j√° que a regulariza√ß√£o torna o aprendizado mais lento e robusto.

-  **Complexidade vs. Performance:** O modelo com **`Mais filtros`** teve um desempenho pior e um n√∫mero de par√¢metros muito maior. Isso refor√ßa a ideia de que "maior nem sempre √© melhor". O aumento da complexidade acelerou o overfitting sem trazer ganhos de performance.

-  **Arquitetura Eficiente:** O modelo **`EfficientStrideCNN`** √© not√°vel por ter um n√∫mero de par√¢metros **extremamente baixo** (apenas 50 mil). Embora sua perda de valida√ß√£o n√£o tenha sido a melhor, as curvas de treino e valida√ß√£o mostram uma tend√™ncia de queda constante, sugerindo que ele poderia se beneficiar muito de um treinamento mais longo. √â uma arquitetura promissora para cen√°rios com restri√ß√µes de recursos.

### Conclus√£o Final

Dentro de um treinamento curto (10 √©pocas), **arquiteturas mais simples se sa√≠ram melhor**, mas mostraram sinais claros de overfitting. T√©cnicas de regulariza√ß√£o como `BatchNorm` e `Dropout` provaram ser eficazes para combater esse problema, embora possam exigir mais tempo de treinamento para atingir seu potencial m√°ximo. A arquitetura `EfficientStrideCNN` destaca-se pela sua efici√™ncia em n√∫mero de par√¢metros, sendo uma excelente op√ß√£o para implanta√ß√£o em dispositivos com hardware limitado.
